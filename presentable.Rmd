---
title: 'Low-Hanging Fruit: Improving Click Revenue'
author: "Madison Hobbs, Vedant Vohra, Alex Gui, David Xu"
date: "4/28/2018"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, cache = TRUE)
```

```{r, include=FALSE}
library(tidyverse)
library(ggthemes)
```


# The Goal

Indeed.com generates revenue from each click a job post gets. Our goal is to help Indeed.com increase revenue generated by clicks, learning where they should put their resources to maximize profit. 

Our data is a non-representative sample of Indeed.com data for legal reasons; Indeed.com could not legally give us complete nor representative data. This fact should most certainly be taken into account when viewing our analysis. Rather than taking the analysis of this data at face-value, we suggest for Indeed.com to run the same analysis on their complete data (or at least a representative subsample).

Thank you to UCLA DataFest for making this project possible and many thanks to Indeed.com for providing us the data. 

# Data Cleaning

To narrow our focus, we look at posts for jobs with locations strictly in the United States. We next look at the pattern or typical "life cycle" for a job post, asking the question: How does the number of clicks vary from day to day for a particular job post?

```{r}
all <- read_csv("2018 DataFest/datafest2018-Updated-April12.csv") %>% filter(country == "US")

all %>% group_by(jobId) %>% mutate()

all %>% group_by(jobId) %>% ggplot(aes(x = jobAgeDays, y = clicks, col = jobId)) + geom_line()
```


Judging by the above plot, most job posts experience the majority of their clicks by the 20th day listed on Indeed.com. To not bias jobs which are left on Indeed.com by employers after the job is filled, we want to restrict our analysis to a period of time where most clicks are generated. 20 days seems like a pretty good cut-off. Therefore, we only look at the first 20 days for each job post. We also leave out job posts which were on Indeed.com for fewer days, so that we are making a fair comparision. 

```{r}
all %>% filter(country == "US") %>% distinct(jobId, .keep_all = TRUE) %>% 
  group_by(normTitleCategory) %>% summarise(c = n()) %>% arrange(desc(c)) %>% head()
  
clean <- all %>%
  filter(country == "US", jobAgeDays < 20) %>% group_by(jobAgeDays) %>% 
  mutate(mean_click_per_day = mean(clicks)) %>%
  mutate(norm_clicks = clicks - mean_click_per_day) 
```

We wish to answer the question: How many more (or fewer) clicks does a given job posting generate as compared to a baseline average? To do this, we develop a ``popularity score." For each day, we compute the mean number of clicks for that day (`jobAgeDay` ranging from 1 to 20) across all job postings. Then, for each job posting, we look at the number of clicks that post got for each `jobAgeDay` and we subtract the mean from 

Why not total the number of clicks a job post got over those 20 days and compare it to the average? This simpler approach is quick and dirty, but does not capture the day-to-day performance. We wanted to develop a metric to reward jobs that do exceptionally better one day . This makes our analysis susceptible to outliers on the job-to-job level, but our goal is to compare jobs within the same industries. The large number of jobs within each industry helps mitigate for day-to-day outliers which makes a certain job post score exceptionally high or low in our metric. We noticed the trend of exponential decay and wanted to reward jobs which defy that general trend or at least do better than average. In this way we count the degree to which a job post is consisently above average or consistently below average. 

Please see the powerpoint presentation for results.

```{r, echo=TRUE, eval = FALSE}
clean <- clean %>% group_by(jobId) %>% mutate(jobScore = sum(norm_clicks))

clean <- clean %>% ungroup() %>% mutate(jobScoreRank = rank(desc(jobScore)))
clean <- clean %>% mutate(money_made = clicks*0.5)
clean_distinct <- clean %>% group_by(jobId) %>% mutate(total_clicks_this_job = sum(clicks), total_money_made_this_job = sum(money_made)) %>% ungroup() %>% distinct(jobId, .keep_all = TRUE) %>% group_by(normTitleCategory) %>% mutate(category_num_jobs = n()) %>% ungroup()

write.csv(clean, "clean.csv")
write.csv(clean_distinct, "clean_distinct.csv")
```


```{r}
clean <- read_csv("clean.csv")
check <- clean %>% group_by(jobId) %>% mutate(total_baseline = sum(mean_click_per_day),total_clicks_this_job = sum(clicks)) %>% ungroup() %>% distinct(jobId, .keep_all = TRUE) %>% group_by(normTitleCategory) %>% mutate(category_num_jobs = n()) %>% ungroup()
```


```{r}
clean_distinct <- read_csv("clean_distinct.csv")
```


```{r, eval=FALSE, include=FALSE}
clean_distinct %>% distinct(jobId, .keep_all = TRUE) %>% filter(category_num_jobs > 100)  %>% group_by(normTitleCategory) %>% summarise(category_mean_score = mean(jobScore), n()) %>% arrange(desc(category_mean_score)) %>% head() 
```
 normTitleCategory category_mean_score `n()`
              <chr>               <dbl> <int>
1             admin            389.6229 16314
2             media            270.7071  1362
3           medinfo            255.5037  2677
4         warehouse            253.0569  7552
5         transport            232.7118  1087
6                hr            218.4816  3993

```{r, eval=FALSE, include=FALSE}
clean %>% summarise(max(jobScoreRank))

clean_distinct %>% filter(category_num_jobs > 1000)  %>% group_by(normTitleCategory) %>% summarise(count = n(), category_mean_score = mean(jobScore), mean_clicks_per_listing =  mean(total_clicks_this_job), total_money = sum(total_money_made_this_job), mean_money_made_per_listing = mean(total_money_made_this_job), rank = mean(jobScoreRank), out_of =5060526) %>% arrange(category_mean_score) %>% head(10)
```

```{r, eval=FALSE, include=FALSE}
clean_distinct %>% filter(category_num_jobs > 1000)  %>% group_by(normTitleCategory) %>% summarise(count = n(), category_mean_score = mean(jobScore), mean_clicks_per_listing =  mean(total_clicks_this_job), total_money = sum(total_money_made_this_job), mean_money_made_per_listing = mean(total_money_made_this_job), rank = mean(jobScoreRank), out_of =5060526) %>% arrange(category_mean_score) %>% filter(normTitleCategory == "management") 
```

```{r, eval=FALSE, include=FALSE}
clean_distinct %>% filter(category_num_jobs > 1000)  %>% group_by(normTitleCategory) %>% summarise(count = n(), category_mean_score = mean(jobScore), mean_clicks_per_listing =  mean(total_clicks_this_job), total_money = sum(total_money_made_this_job), mean_money_made_per_listing = mean(total_money_made_this_job), rank = mean(jobScoreRank), out_of =5060526) %>% arrange(desc(category_mean_score)) %>% head() 

the_tops <- clean_distinct %>% filter(category_num_jobs > 1000)  %>% group_by(normTitleCategory) %>% summarise(count = n(), category_mean_score = mean(jobScore), mean_clicks_per_listing =  mean(total_clicks_this_job), se_clicks_per_listing =  sd(total_clicks_this_job)/sqrt(count), total_money = sum(total_money_made_this_job), mean_money_made_per_listing = mean(total_money_made_this_job), rank = mean(jobScoreRank), out_of =5060526) %>% arrange(desc(category_mean_score))

the_tops
```

Good: admin         278
customer      126
warehouse      98
protective     41
management     37

Bad: mednurse      134
management    109
food           95
retail         87
driver         74

```{r, eval=FALSE, include=FALSE}
library(wordcloud)
```


out of the first 1000 worst/best jobs (in terms of cumulative score), how many belong to which industry.

```{r, eval=FALSE, include=FALSE}
freq_top_1000 <- c(278, 126, 98, 41, 37, 134, 109, 95, 87, 74)
normTitleCategory <- c("admin", "customer", "warehouse", "protective", "management", "mednurse", "management", "food", "retail", "driver")
best_or_worst <- c("best", "best", "best", "best", "best", "worst", "worst", "worst","worst","worst")
toadd <- data.frame(freq_top_1000, normTitleCategory, best_or_worst)

together <- inner_join(the_tops, toadd, by = c("normTitleCategory"))

together %>% filter(best_or_worst == "best") %>% mutate(normTitleCategory = factor(normTitleCategory, levels = normTitleCategory[order(desc(freq_top_1000))])) %>%
  ggplot(aes(x=normTitleCategory, y=freq_top_1000, fill = category_mean_score)) + 
  geom_point(size=3) +
  geom_segment(aes(x=normTitleCategory, 
                   xend=normTitleCategory, 
                   y=0, 
                   yend=freq_top_1000)) + 
  labs(title="Lollipop Chart", 
       subtitle="Make Vs Avg. Mileage", 
       caption="source: mpg") + 
  theme(axis.text.x = element_text(angle=65, vjust=0.6))
```

```{r, eval=FALSE, include=FALSE }
cor(the_tops$category_mean_score, the_tops$mean_clicks_per_listing)
```


```{r, eval=FALSE, include=FALSE}
together %>% filter(best_or_worst == "worst") %>% mutate(normTitleCategory = factor(normTitleCategory, levels = normTitleCategory[order(desc(freq_top_1000))])) %>%
  ggplot(aes(x=normTitleCategory, y=freq_top_1000, fill = category_mean_score)) + 
  geom_point(size=3) +
  geom_segment(aes(x=normTitleCategory, 
                   xend=normTitleCategory, 
                   y=0, 
                   yend=freq_top_1000)) + 
  labs(title="Lollipop Chart", 
       subtitle="Make Vs Avg. Mileage", 
       caption="source: mpg") + 
  theme(axis.text.x = element_text(angle=65, vjust=0.6))
```


```{r, eval=FALSE, include=FALSE}
the_tops %>% mutate(popularity = case_when(
  category_mean_score > 0 ~ "above average",
  category_mean_score <= 0 ~ "below average"
), normTitleCategory = factor(normTitleCategory, levels = normTitleCategory[order(category_mean_score)])) %>% 
  filter(!is.na(normTitleCategory)) %>%
  ggplot(aes(x=normTitleCategory, y=category_mean_score, label=round(category_mean_score, 0))) + 
  geom_point(stat='identity', aes(col=popularity), size=6)  + 
  geom_text(color="white", size=2) +
  labs(title="Diverging Dot Plot", 
       subtitle="popularity score and job sector") +
  coord_flip() + geom_segment(aes(y = 0, 
                   x = normTitleCategory, 
                   yend = category_mean_score, 
                   xend =  normTitleCategory), 
               color = "black")
```


```{r, eval=FALSE, include=FALSE}
the_tops %>% filter(normTitleCategory %in% c("admin", "media", "medinfo", "warehouse", "transport", "management",
                                             "retail", "sales", "food", "mednurse"))
```


```{r, eval=FALSE, include=FALSE}
library(ggthemes)



the_tops %>% filter(normTitleCategory %in% c("admin", "media", "medinfo", "warehouse", "transport", "mednurse", "management",
                                             "retail", "food", "sales")) %>%
  mutate(normTitleCategory = c("admin", "media", "medinfo", "warehouse", "transport", "mednurse", "mgmt",
                                             "retail","food", "sales"),
    rank = c(1,2,3,4,5,1,2,3,4,5), selection = c("most popular", "most popular", "most popular", "most popular", "most popular",
         "most job posts", "most job posts", "most job posts", "most job posts", "most job posts"),
         normTitleCategory = factor(normTitleCategory, levels = normTitleCategory[order(rank)]),
    ) %>%
  ggplot(aes(x = rank, y = mean_clicks_per_listing, fill = selection, ymin = mean_clicks_per_listing - 2.6*se_clicks_per_listing, ymax = mean_clicks_per_listing + 2.6*se_clicks_per_listing, label = normTitleCategory)) + geom_bar(position = position_dodge(), stat="identity") + theme_economist() + scale_fill_manual(values = c("red4", "mediumseagreen")) + geom_text(
    aes(label = normTitleCategory, y = mean_clicks_per_listing + 73),
    position = position_dodge(0.9),
    vjust = 0, fontface = "bold", size = 7,
  ) +xlab("Best to Worst") + ylab("Average Clicks Per Job Post") + theme(text = element_text(size=20, face = "bold")) + geom_errorbar(position = position_dodge(width = 0.9), width=0.2,alpha=0.9, size=1.3)


 + geom_text(aes(fill = selection), colour = "white", fontface = "bold")

+ theme(axis.title.x = element_text(vjust=-5) , 
          plot.margin = (unit(c(.5, .5, 2, .5), "cm"))) + 

the_tops$name <- factor(x$name, levels = x$name[order(x$val)])

the_tops
```


```{r, eval=FALSE, include=FALSE}
ggplot(filter(the_tops, normTitleCategory %in% c(admin, mednurse)), aes(x = ))
```


```{r, eval=FALSE, include=FALSE}
revenue_makers <- clean %>% filter(normTitleCategory %in% c("admin", "media", "medinfo", "warehouse", "transport")) %>% group_by(normTitleCategory, jobAgeDays) %>% mutate(mean_clicks_per_day_category = mean(clicks))

ggplot(revenue_makers, aes(x = jobAgeDays, y = mean_clicks_per_day_category, col = normTitleCategory, alpha = )) + geom_line() + geom_line(aes(y = mean_click_per_day), col = "black", size = 2) + theme_minimal() + scale_color_brewer(palette = "greens")
```

```{r, eval=FALSE, include=FALSE}
most_prevalent <- clean %>% filter(normTitleCategory %in% c("mednurse", "management", "retail", "food", "sales")) %>% group_by(normTitleCategory, jobAgeDays) %>% mutate(mean_clicks_per_day_category = mean(clicks))

ggplot(most_prevalent, aes(x = jobAgeDays, y = mean_clicks_per_day_category, col = normTitleCategory)) + geom_line() + geom_line(aes(y = mean_click_per_day), col = "black", size = 2)
```

```{r}

```

